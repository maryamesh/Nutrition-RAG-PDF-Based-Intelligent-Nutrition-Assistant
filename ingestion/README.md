# ğŸ“„ PDF Ingestion & Text Utilities

This section explains **`ingest_pdf.py`** and **`utils.py`**, the two foundational files responsible for transforming raw PDF documents into clean, structured, and RAG-ready data.

These files work **before embeddings, vector databases, or LLMs** and form the backbone of the entire pipeline.

---

## ğŸ§  High-Level Overview

| File | Purpose |
|-----|--------|
| ğŸ—‚ï¸ **ingest_pdf.py** | Controls the **document ingestion pipeline** (PDF â†’ chunks) |
| ğŸ› ï¸ **utils.py** | Provides **text processing utilities** (cleaning, splitting, chunking, prompting) |

ğŸ‘‰ **Key idea:**  
`ingest_pdf.py` orchestrates the workflow, while `utils.py` contains the reusable intelligence.

---

## ğŸ“˜ ingest_pdf.py â€” PDF Ingestion Pipeline

### ğŸ¯ What this file does

`ingest_pdf.py` is responsible for converting a **raw PDF document** into **structured semantic chunks** that are ready for embedding and retrieval.

It performs the following steps:

1. ğŸ“¥ Download the PDF (if not available locally)
2. ğŸ“„ Read the PDF page-by-page
3. ğŸ§¹ Clean extracted text
4. âœ‚ï¸ Split text into sentences
5. ğŸ§© Group sentences into meaningful chunks
6. ğŸš« Remove tiny / noisy chunks
7. ğŸ’¾ Save final chunks to disk (`parquet`)

---

## ğŸ”„ Pipeline Flow

```text
PDF
 â†“
Page-level text extraction
 â†“
Sentence splitting (spaCy)
 â†“
Sentence chunking
 â†“
Noise filtering
 â†“
chunks.parquet (RAG-ready)
```

---

### ğŸ§© Key Functions

#### `download_pdf()`
- Downloads the PDF only if it doesnâ€™t exist
- Ensures reproducibility and automation

#### `open_and_read_pdf()`
- Extracts text per page using PyMuPDF
- Computes page-level statistics (tokens, words, sentences)

#### `add_sentences_to_pages()`
- Applies linguistic sentence segmentation
- Adds sentence lists to each page

#### `build_chunks_from_pages()`
- Groups sentences into paragraph-like chunks
- Preserves semantic meaning

#### `ingest_pdf()`
- Orchestrates the full ingestion pipeline
- Saves the final output to `chunks.parquet`

---

### ğŸ“¦ Output

The output is a **structured dataset** where each row represents a meaningful text chunk with metadata:

- Page number
- Clean chunk text
- Token / word / character statistics

This file **never talks to embeddings, Pinecone, or LLMs**.

---

## ğŸ› ï¸ utils.py â€” Text Processing & Prompt Utilities

### ğŸ¯ What this file does

`utils.py` contains all **reusable helper functions** related to text processing and prompt construction.

It focuses on:
- Cleaning text
- Linguistic sentence splitting
- Semantic chunk creation
- Noise filtering
- RAG prompt formatting

---

### ğŸ§  Key Functions Explained

#### ğŸ§¹ `text_formatter()`
- Cleans raw PDF text
- Removes line breaks and spacing artifacts

#### âœ‚ï¸ `split_sentences_spacy()`
- Uses spaCyâ€™s sentencizer
- Produces clean sentence-level splits
- More reliable than regex-based splitting

#### ğŸ§© `create_sentence_chunks()`
- Groups sentences into fixed-size semantic chunks
- Preserves meaning and structure
- Computes chunk-level statistics

#### ğŸš« `filter_chunks()`
- Removes very small or irrelevant chunks
- Eliminates headers, footers, and noise

#### ğŸ§  `prompt_formatter()`
- Builds a **structured RAG prompt**
- Injects retrieved context
- Enforces grounded, detailed answers
- Prevents hallucinations

---

## âš–ï¸ Difference Between ingest_pdf.py & utils.py

| Aspect | ingest_pdf.py | utils.py |
|-----|--------------|---------|
| Role | Pipeline controller | Utility functions |
| Responsibility | Orchestration | Text intelligence |
| Handles files | âœ… Yes | âŒ No |
| Reusable across projects | âŒ Mostly | âœ… Yes |
| Knows about PDFs | âœ… Yes | âŒ No |
| Knows about RAG prompting | âŒ No | âœ… Yes |

---

## ğŸ”— How They Work Together

```python
ingest_pdf.py
 â”œâ”€ calls text_formatter()
 â”œâ”€ calls split_sentences_spacy()
 â”œâ”€ calls create_sentence_chunks()
 â””â”€ calls filter_chunks()

utils.py
 â””â”€ provides all text-processing logic

